{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6579775e-ad9d-405b-b8cd-4812b1d163ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Unzip the dataset\n",
    "!unzip /content/drive/MyDrive/radar_data_unica_2018_2023_sorted.zip -d /content/radar_data_unica_2018_2023_sorted\n",
    "\n",
    "# Clone the GitHub repository for the models and utilities\n",
    "!git clone https://github.com/AVI68/UNet_ConvLSTM.git\n",
    "\n",
    "# Install any dependencies if \n",
    "\n",
    "!pip install -r /content/UNet_ConvLSTM/requirements.txt\n",
    "# Install necessary TensorFlow \n",
    "!pip install tensorflow\n",
    "\n",
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import utility_tf as utility\n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from models.UNet_ConvLSTM import unet_convlstm_reg\n",
    "import gc  # Garbage collector\n",
    "\n",
    "# Set paths for Google Colab\n",
    "data_dir = \"/content/radar_data_unica_2018_2023_sorted\"\n",
    "excel_file = \"/content/UNet_ConvLSTM/image_isw_scores.xlsx\"\n",
    "output_dir = \"/content/drive/MyDrive/UNet_ConvLSTM\" \n",
    "log_dir = os.path.join(output_dir, 'logs')\n",
    "checkpoint_dir = os.path.join(output_dir, 'checkpoints')\n",
    "\n",
    "# Create necessary directories\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "class RadarDatasetTF:\n",
    "    \"\"\"Custom Radar dataset for loading and processing radar images for TensorFlow.\"\"\"\n",
    "    def __init__(self, times, base_dir, input_steps=16, output_steps=15):\n",
    "        self.times = times\n",
    "        self.base_dir = base_dir\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.times) - (self.input_steps + self.output_steps)\n",
    "\n",
    "    def _load_sample(self, idx):\n",
    "        current_time = self.times.iloc[idx]\n",
    "        images = np.zeros((self.input_steps + self.output_steps, 256, 256), dtype=np.float32)\n",
    "        \n",
    "        for i in range(-(self.input_steps - 1), (self.output_steps + 1), 1):\n",
    "            time_step = current_time + pd.Timedelta(minutes=i)\n",
    "            filename = utility.fname2dt(time_step, inverse=True)\n",
    "            file_path = os.path.join(self.base_dir, filename)\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                image = utility.read_image(file_path)\n",
    "                image = utility.normalize(image, inverse=False)\n",
    "                images[i + (self.input_steps - 1), :, :] = image  # Adjust index for correct placement\n",
    "                \n",
    "        x = images[:self.input_steps, :, :].reshape(self.input_steps, 256, 256, 1)\n",
    "        y = images[self.input_steps:, :, :].reshape(self.output_steps, 256, 256, 1)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "    def to_tf_dataset(self, batch_size=32, shuffle=True):\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(np.arange(len(self)))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=len(self))\n",
    "        dataset = dataset.map(lambda idx: tf.numpy_function(\n",
    "            func=self._load_sample, inp=[idx], Tout=(tf.float32, tf.float32)),\n",
    "            num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        return dataset\n",
    "\n",
    "# Prepare datasets\n",
    "times = pd.to_datetime(df.iloc[:, 0])\n",
    "train_times = times[times.dt.year.isin([2018, 2020, 2021, 2022, 2023])]\n",
    "valid_times = times[(times.dt.year == 2019) & (times.dt.month <= 9)]\n",
    "test_times = times[(times.dt.year == 2019) & (times.dt.month >= 10)]\n",
    "\n",
    "train_dataset = RadarDatasetTF(train_times, data_dir)\n",
    "valid_dataset = RadarDatasetTF(valid_times, data_dir)\n",
    "test_dataset = RadarDatasetTF(test_times, data_dir)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = train_dataset.to_tf_dataset(batch_size=BATCH_SIZE)\n",
    "valid_loader = valid_dataset.to_tf_dataset(batch_size=BATCH_SIZE)\n",
    "test_loader = test_dataset.to_tf_dataset(batch_size=BATCH_SIZE)\n",
    "\n",
    "def train_model(loss_function_name):\n",
    "    \"\"\"\n",
    "    Train the model using the specified loss function.\n",
    "    \n",
    "    Parameters:\n",
    "        loss_function_name (str): The name of the loss function to use ('MCS_loss' or 'CB_loss').\n",
    "    \"\"\"\n",
    "    # Select the loss function based on the parameter\n",
    "    if loss_function_name == 'MCS_loss':\n",
    "        loss_function = utility.MCS_loss\n",
    "    elif loss_function_name == 'CB_loss':\n",
    "        loss_function = utility.CB_loss\n",
    "    elif loss_function_name == 'Bmae_loss':\n",
    "        loss_function = utility.Bmae_loss\n",
    "    elif loss_function_name == 'Bmse_loss':\n",
    "        loss_function = utility.Bmse_loss\n",
    "    elif loss_function_name == 'mse_loss':\n",
    "        loss_function = 'mse'\n",
    "    else:\n",
    "        raise ValueError(\"Invalid loss function name. Choose 'MCS_loss','CB_loss','Bmae_loss','Bmse_loss','mse_loss'.\")\n",
    "    \n",
    "    # Define model\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = unet_convlstm_reg(input_shape=(16, 256, 256, 1), num_filters_base=16, dropout_rate=0.2, seq_len=15)\n",
    "        model.compile(optimizer=Adam(learning_rate=0.0001), loss=loss_function)\n",
    "    \n",
    "    # Callbacks\n",
    "    tensorboard_callback = TensorBoard(log_dir=os.path.join(log_dir, 'experiment_1', loss_function_name))\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_dir, f'model_{loss_function_name}_epoch_{{epoch:02d}}.h5'),\n",
    "        save_weights_only=False,\n",
    "        save_freq='epoch'\n",
    "    )\n",
    "    csv_logger = CSVLogger(os.path.join(log_dir, f'training_log_{loss_function_name}.csv'))\n",
    "\n",
    "    # Training\n",
    "    n_epochs = 4\n",
    "    with tf.device('/GPU:0'):\n",
    "        history = model.fit(\n",
    "            train_loader,\n",
    "            epochs=n_epochs,\n",
    "            validation_data=valid_loader,\n",
    "            callbacks=[tensorboard_callback, checkpoint_callback, csv_logger]\n",
    "        )\n",
    "\n",
    "    # Clear memory after training\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "    # Save the final model\n",
    "    model.save(os.path.join(output_dir, f'unet_convlstm_{loss_function_name}-final.h5'))\n",
    "\n",
    "    # Evaluate on the test set\n",
    "    with tf.device('/GPU:0'):\n",
    "        test_loss = model.evaluate(test_loader)\n",
    "        print(f\"Test loss with {loss_function_name}: {test_loss:.6f}\")\n",
    "\n",
    "    # Clear memory after evaluation\n",
    "    tf.keras.backend.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "# Train with MCS_loss and CB_loss\n",
    "train_model('MCS_loss')\n",
    "train_model('CB_loss')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
