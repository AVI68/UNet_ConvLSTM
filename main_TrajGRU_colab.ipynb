{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97cbcb-463d-478f-a130-3225d9884852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing google drive and mount the drive content\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# unzip my data file\n",
    "!unzip /content/drive/MyDrive/radar_data_unica_2018_2023_sorted.zip -d /content/radar_data_unica_2018_2023_sorted\n",
    "\n",
    "# clone git directory \n",
    "!git clone https://github.com/AVI68/UNet_ConvLSTM.git\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Unzip the dataset\n",
    "!unzip /content/drive/MyDrive/radar_data_unica_2018_2023_sorted.zip -d /content/radar_data_unica_2018_2023_sorted\n",
    "\n",
    "# Clone the GitHub repository for the models\n",
    "!git clone https://github.com/AVI68/UNet_ConvLSTM.git\n",
    "\n",
    "# Install any dependencies \n",
    "\n",
    "!pip install -r /content/UNet_ConvLSTM/requirements.txt\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import utility  \n",
    "from models.u_net import UNet\n",
    "from models.naive_cnn import cnn_2D\n",
    "from models.traj_gru import TrajGRU\n",
    "from models.conv_gru import ConvGRU\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# RadarDataset and train_network functions are already provided by you\n",
    "\n",
    "class RadarDataset(Dataset):\n",
    "    \"\"\"Custom Radar dataset for loading and processing radar images.\"\"\"\n",
    "    def __init__(self, times, base_dir, input_steps=16, output_steps=15, recurrent_nn=False):\n",
    "        self.times = times\n",
    "        self.base_dir = base_dir\n",
    "        self.input_steps = input_steps\n",
    "        self.output_steps = output_steps\n",
    "        self.recurrent_nn = recurrent_nn\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.times) - (self.input_steps + self.output_steps)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_time = self.times.iloc[idx]\n",
    "        images = np.zeros((self.input_steps + self.output_steps, 256, 256), dtype=np.float32)\n",
    "        \n",
    "        for i in range(-(self.input_steps - 1), (self.output_steps + 1), 1):\n",
    "            time_step = current_time + pd.Timedelta(minutes=i * 5)\n",
    "            filename = utility.fname2dt(time_step, inverse=True)\n",
    "            file_path = os.path.join(self.base_dir, filename)\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                image = utility.read_image(file_path)\n",
    "                image = utility.normalize(image, inverse=False)\n",
    "                images[i + (self.input_steps - 1), :, :] = image  # Adjust index for correct placement\n",
    "                \n",
    "        if self.recurrent_nn:\n",
    "            # For recurrent models, we add an extra dimension for the channel (which is 1 for rain maps)\n",
    "            x = images[:self.input_steps, :, :].reshape(self.input_steps, 1, 256, 256)\n",
    "            y = images[self.input_steps:, :, :].reshape(self.output_steps, 1, 256, 256)\n",
    "        else:\n",
    "            # For non-recurrent models, the input is typically a sequence of images without an additional channel dimension\n",
    "            x = images[:self.input_steps, :, :]\n",
    "            y = images[self.input_steps:, :, :]\n",
    "        \n",
    "        # Convert numpy arrays to PyTorch tensors\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "        return x, y\n",
    "\n",
    "def train_network(network, train_loader, valid_loader, loss_type, epochs, batch_size, device, log_dir, print_metric_logs=False):\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    n_examples_train = len(train_loader.dataset)\n",
    "    n_examples_valid = len(valid_loader.dataset)\n",
    "\n",
    "    lr = 1e-5\n",
    "    wd = 0.1\n",
    "    optimizer = torch.optim.Adam(network.parameters(), lr=lr, weight_decay=wd)\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1)\n",
    "\n",
    "    info = f'''Starting training:\n",
    "        Epochs:                {epochs},\n",
    "        Learning rate:         {lr},\n",
    "        Batch size:            {batch_size},\n",
    "        Weight decay:          {wd},\n",
    "        Number batch train :   {len(train_loader)},\n",
    "        Number batch val :     {len(valid_loader)},\n",
    "        Scheduler :            Gamma 0.1 epochs 30, 60\n",
    "    '''\n",
    "    writer.add_text('Description', info)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        network.train()\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        loop = tqdm(train_loader)\n",
    "        loop.set_description(f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        writer.add_scalar('LR', scheduler.get_last_lr()[0], epoch)\n",
    "\n",
    "        for batch_idx, (inputs, targets) in enumerate(loop):\n",
    "            inputs = inputs.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = network(inputs)\n",
    "            if loss_type == 'CB_loss':\n",
    "                loss = utility.CB_loss(outputs, targets)\n",
    "            elif loss_type == 'MCS_loss':\n",
    "                loss = utility.MCS_loss(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item() / n_examples_train\n",
    "\n",
    "            loop.set_postfix({'Train Loss': training_loss})\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        network.eval()\n",
    "\n",
    "        for inputs, targets in valid_loader:\n",
    "            inputs = inputs.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            outputs = network(inputs)\n",
    "            if loss_type == 'CB_loss':\n",
    "                loss = utility.CB_loss(outputs, targets)\n",
    "            elif loss_type == 'MCS_loss':\n",
    "                loss = utility.MCS_loss(outputs, targets)\n",
    "            validation_loss += loss.item() / n_examples_valid\n",
    "\n",
    "        writer.add_scalar('Loss/train', training_loss, epoch)\n",
    "        writer.add_scalar('Loss/test', validation_loss, epoch)\n",
    "        print(f\"[Validation] Loss: {validation_loss:.2f}\")\n",
    "\n",
    "        torch.save(network, log_dir + f'/model_{epoch+1}.pth')\n",
    "\n",
    "# Load the data\n",
    "excel_file = \"/content/UNet_ConvLSTM/image_isw_scores.xlsx\"\n",
    "df = pd.read_excel(excel_file)\n",
    "times = pd.to_datetime(df.iloc[:, 0])\n",
    "\n",
    "# Filter data for training, validation, and testing\n",
    "train_times = times[times.dt.year.isin([2018, 2020, 2021, 2022, 2023])]\n",
    "valid_times = times[(times.dt.year == 2019) & (times.dt.month <= 9)]\n",
    "test_times = times[(times.dt.year == 2019) & (times.dt.month >= 10)]\n",
    "\n",
    "# Common parameters\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data_dir = \"/content/radar_data_unica_2018_2023_sorted\"\n",
    "epochs = 4\n",
    "batch_size = 8\n",
    "input_length = 16\n",
    "output_length = 15\n",
    "\n",
    "# Iterate through all combinations of models and loss functions\n",
    "models = {\n",
    "    'ConvGRU': ConvGRU(device=device, input_length=input_length, output_length=output_length),\n",
    "    'TrajGRU': TrajGRU(device=device, input_length=input_length, output_length=output_length),\n",
    "    'CNN2D': cnn_2D(input_length=input_length, output_length=output_length, filter_number=64),\n",
    "    'UNet': UNet(input_length=input_length, output_length=output_length, filter_number=64),\n",
    "}\n",
    "\n",
    "loss_functions = ['CB_loss', 'MCS_loss']\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    for loss_type in loss_functions:\n",
    "        log_dir = f\"/content/drive/MyDrive/run/{model_name}_{loss_type}\"\n",
    "        \n",
    "        # Determine if the model is recurrent\n",
    "        recurrent_nn = model_name in ['ConvGRU', 'TrajGRU']\n",
    "        \n",
    "        # Create datasets and data loaders\n",
    "        train_dataset = RadarDataset(train_times, data_dir, input_steps=input_length, output_steps=output_length, recurrent_nn=recurrent_nn)\n",
    "        valid_dataset = RadarDataset(valid_times, data_dir, input_steps=input_length, output_steps=output_length, recurrent_nn=recurrent_nn)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Move model to the appropriate device\n",
    "        model.to(device)\n",
    "        \n",
    "        # Train the network\n",
    "        train_network(model, train_loader, valid_loader, loss_type, epochs, batch_size, device, log_dir, print_metric_logs=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
